---
title: "STA640 Homework 2"
author: "Fan Bu"
date: "9/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
```{r, message=FALSE, warning=FALSE}
library(tableone)
library(survey)
library(reshape2)
library(ggplot2)
library(tidyverse)
data = read_delim('HW2_data.txt', delim=' ',
                  col_types = 'fnffffffnnnf')
```

### (a)
Estimate propensity score with logistic regression using all covariates as main effects (note that I've tried adding higher-order terms, but it tends to hinder algorithm convergence and doesn't improve balance by much). Then obtain the inverse probability weights.
```{r, message=FALSE}
# centering the continuous variables helps...
data[,c('i_age','com_t','pcs_sd','mcs_sd')] = scale(data[,c('i_age','com_t','pcs_sd','mcs_sd')],scale=FALSE)

ps.model = glm(pg ~ i_age + i_sex + i_race + i_educ + 
                 i_insu + i_drug + i_seve + com_t + 
                 pcs_sd + mcs_sd, 
               data=data, family=binomial('logit'))
```

```{r, echo=FALSE}
ps.model = readRDS('ps_model.rds')
```

```{r}
ps = ps.model$fitted.values %>% as.numeric()

data$ipw = 1
data$ipw[data$pg == 2] = 1/ps[data$pg == 2]
data$ipw[data$pg == 1] = 1/(1-ps[data$pg == 1])

data$ow = 1
data$ow[data$pg == 2] = 1 - ps[data$pg == 2]
data$ow[data$pg == 1] = ps[data$pg == 1]
```

Examine covariate balance (here I'm using the unnormalized ASD version, with commonly used threshold 0.1). Here we have achieved good balance (ASD < 0.1) for some of the covariates, but for the others, there is still imbalance.

Note that there does exist a couple of subjects (subject 5 and 130) with extremely low ($<0.001$) or extremely high ($>0.999$) estimated propensity scores. And here I will exclude those extreme subjects. 
```{r}
## Weighted data
dataIPW <- svydesign(ids = ~ 1, 
                     data = data[ps > 0.001 & ps < 0.999,], 
                     weights = ~ ipw)

vars = names(data)[2:11]

## Construct a table that checks balance
tabWeighted <- svyCreateTableOne(vars = vars, strata = 'pg',
                                 data = dataIPW, test = FALSE)
## Show table with SMD
print(tabWeighted, smd = TRUE)
```


### (b)

Below is the histogram of estimated propensity scores by group.

```{r, echo=FALSE}
data$ps = ps

ggplot(data, aes(x=ps, fill=pg)) +
  geom_histogram(bins=30, position='dodge') +
  labs(fill='physician\ngroup', x='propensity score')
```

And also the Love plot showing ASD before and after matching.

```{r, echo=FALSE}
# nothing
tabUnmatched <- CreateTableOne(vars = vars, strata = "pg", data = data, test = FALSE)

## Weighted data (OW)
dataOW <- svydesign(ids = ~ 1, data = data, weights = ~ ow)

## Construct a table that checks balance
tabWeightedOW <- svyCreateTableOne(vars = vars, strata = 'pg',
                                 data = dataOW, test = FALSE)

```


```{r, echo=FALSE}
## Construct a data frame containing variable name and SMD from all methods
dataPlot <- data.frame(variable  = rownames(ExtractSmd(tabUnmatched)),
                       Unweighted = as.numeric(ExtractSmd(tabUnmatched)),
                       IPW  = as.numeric(ExtractSmd(tabWeighted)),
                       OW = as.numeric(ExtractSmd(tabWeightedOW)))

## Create long-format data for ggplot2
dataPlotMelt <- melt(data          = dataPlot,
                     id.vars       = c("variable"),
                     variable.name = "Method",
                     value.name    = "SMD")

## Order variable names by magnitude of SMD
varNames <- as.character(dataPlot$variable)[order(dataPlot$Unweighted)]

## Order factor levels in the same order
dataPlotMelt$variable <- factor(dataPlotMelt$variable,
                                levels = varNames)

## Plot using ggplot2
ggplot(data = dataPlotMelt,
       mapping = aes(x = variable, y = SMD, group = Method, color = Method)) +
    #geom_line() +
    geom_hline(yintercept = 0.1, color = "black", size = 0.3) +
    geom_point() +
    coord_flip() +
    theme_bw() + theme(legend.key = element_blank())
```


We can see that the original data (before weighting) do not have good balance on most of the covariates, in particular `race` and `comorbidity`, but there is some overlap between the two groups. After weighting, we achieve better balance in the covariates. 

### (c)

(The table generated in R Markdown is a bit ugly.. Sorry about that.)

```{r get weighted average function, echo=FALSE}
weight_avg <- function(x, w, varname=NULL){
  if(is.numeric(x)){
    # continuous variable
    res = sum(w*x)/sum(w)
    names(res) = varname
    res
  }else{
    res = NULL
    names = NULL
    for(l in levels(x)){
      freq.l = sum((x==l) * w)/sum(w)
      res = c(res, freq.l)
      names = c(names, paste0(varname,'=',l))
    }
    names(res) = names
    res
  }
}

get_var_wa <- function(dat, varname){
  res = NULL
  
  x = dat[,varname,drop=TRUE]
  n = nrow(dat)
  
  # unweighted
  uni_W = rep(1,n)
  res = cbind(res, weight_avg(x[dat$pg==1], uni_W[dat$pg==1], varname))
  res = cbind(res, weight_avg(x[dat$pg==2], uni_W[dat$pg==2], varname))
  res = cbind(res, weight_avg(x, uni_W, varname))
  
  # IPW
  res = cbind(res, weight_avg(x, dat$ipw, varname))
  
  # OW
  res = cbind(res, weight_avg(x, dat$ow, varname))
  
  res
}

```


```{r construct the weighted average table, echo=FALSE}
tab = NULL

data_orig = read_delim('HW2_data.txt', delim=' ',
                  col_types = 'fnffffffnnnf')

# get original continuous columns
data_orig$ipw = data$ipw
data_orig$ow = data$ow
data_orig$ps = data$ps

for(v in vars){
  tab = rbind(tab, get_var_wa(data_orig, v))
}

tab = as.data.frame(tab)
names(tab) = c('PG 1 (unadj.)', 'PG 2 (unadj.)', 
               'Overall (unadj.)', 'IPW (adjusted)', 
               'OW (adjusted)')
knitr::kable(tab,digits=3)
```


<!-- |     |     |Unadjusted |       |    Adjusted |   | -->
<!-- |-----|------|-------|-------|-------|----------| -->
<!-- |     | **PG 1** | **PG 2** | **Overall** |  **IPW** |  **OW**  | -->
<!-- | Age (yrs) |  40.45 | 39.65 |        |  -->

## (d)

The table below summarizes ATE and ATO estimates (with un-normalized and normalized weights for ATE). Here the standard errors are calculated via bootstrap.

ATE and ATO are different because they focus on different target populations. The former focuses on the entire (sample) population while the latter has much more emphasis on the overlapping portion of the sample population. 

Within a weighting scheme, we can see that, for ATE (with IPW), when using normalized weights, the standard errors are relatively lower. (When using overlap weights, we should **always** normalize the weights, so we don't do an un-normalized version here.)
<!-- though this isn't exactly the case for ATO, where we don't see much difference in the (relative) scale of the standard errors.  -->

```{r, echo=FALSE}
get_weighted_tau <- function(y, z, w, normalize=TRUE){
  if(normalize){
    tau = sum((y[z==1]==1) * w[z==1])/ sum(w[z==1]) - 
      sum((y[z==2]==1) * w[z==2])/ sum(w[z==2])
  }else{
    n = length(y)
    tau = (sum((y[z==1]==1) * w[z==1]) - sum((y[z==2]==1) * w[z==2]))/n
  }
  tau
}

# get_sd <- function(y,z,w,B=5000,normalize=TRUE){
#   n = length(y)
#   boot = sapply(1:B, function(i) {
#     samp = sample(n,replace = TRUE)
#     get_weighted_tau(y[samp],z[samp],w[samp],normalize)
#   })
#   #print(boot)
#   sd(boot)
# }

get_sd <- function(d, ps_formula, weight='ipw', 
                   normalize=TRUE, B=1000, subset=NULL){
  n = nrow(d)
  inds1 = which(d$pg == 1); n1 = length(inds1)
  inds2 = which(d$pg == 2); n2 = length(inds2)
  boot = sapply(1:B, function(i) {
    d.samp = d[sample(n,replace = TRUE),]
    #d.samp = d[c(sample(inds1, replace=TRUE), sample(inds2, replace=TRUE)),]
    #d.samp = d.samp %>% distinct()
    
    # re-adjust the factors
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x)) factor(x) else x)
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x) & length(levels(x))<2) as.numeric(x) else x)
    
    ps = glm(ps_formula, data=d.samp, family=binomial('logit'))$fitted.values %>% as.numeric()

    d.samp$ipw = 1
    d.samp$ipw[d.samp$pg == 2] = 1/ps[d.samp$pg == 2]
    d.samp$ipw[d.samp$pg == 1] = 1/(1-ps[d.samp$pg == 1])

    d.samp$ow = 1
    d.samp$ow[d.samp$pg == 2] = 1 - ps[d.samp$pg == 2]
    d.samp$ow[d.samp$pg == 1] = ps[d.samp$pg == 1]
    
    if (is.null(subset)){
      get_weighted_tau(d.samp$i_aqoc, d.samp$pg, 
                     d.samp[, weight, drop=TRUE], normalize)
    }else{
      d.samp = d.samp[subset,]
      get_weighted_tau(d.samp$i_aqoc, d.samp$pg, 
                     d.samp[, weight, drop=TRUE], normalize)
    }
    
  })
  sd(boot)
}

```


```{r,echo=FALSE,eval=FALSE,warning=FALSE, message=FALSE}
ff = ps.model$formula

ATE = NULL
ATE = c(ATE, get_weighted_tau(data$i_aqoc, data$pg, data$ipw, normalize = FALSE))
#ATE = c(ATE, get_sd(data$i_aqoc, data$pg, data$ipw, normalize = FALSE))
ATE = c(ATE, get_sd(data, ff, "ipw", normalize = FALSE))
ATE = c(ATE, get_weighted_tau(data$i_aqoc, data$pg, data$ipw, normalize = TRUE))
#ATE = c(ATE, get_sd(data$i_aqoc, data$pg, data$ipw, normalize = TRUE))
ATE = c(ATE, get_sd(data, ff, "ipw", normalize = TRUE))

ATO = NULL
ATO = c(ATO, get_weighted_tau(data$i_aqoc, data$pg, data$ow, normalize = FALSE))
#ATO = c(ATO, get_sd(data$i_aqoc, data$pg, data$ow, normalize = FALSE))
ATO = c(ATO, get_sd(data, ff, "ow", normalize = FALSE))
ATO = c(ATO, get_weighted_tau(data$i_aqoc, data$pg, data$ow, normalize = TRUE))
#ATO = c(ATO, get_sd(data$i_aqoc, data$pg, data$ow, normalize = TRUE))
ATO = c(ATO, get_sd(data, ff, "ow", normalize = TRUE))

res = rbind(ATE, ATO)
res = as.data.frame(res)
names(res) = c('est. (un-normalized)', 'sd (un-normalized)',
               'est. (normalized)', 'sd (normalized)')
```

```{r,echo=FALSE}
res = readRDS('ATE_ATO_res2.rds')
res[2,1:2] = NA
knitr::kable(res,digits = 3)
```

## Problem 2

In this problem, all standard deviations are calculated via bootstrap, where in each iteration the PS model is re-fitted. 

### (a) 

Estimate PS with `sex` included, and then do IPW.
```{r}
ps.model = glm(pg ~ i_age + i_sex + i_race + i_educ + 
                 i_insu + i_drug + i_seve + com_t + 
                 pcs_sd + mcs_sd, 
               data=data, family=binomial('logit'))
data$ps = ps.model$fitted.values %>% as.numeric()

data$ipw = 1
data$ipw[data$pg == 2] = 1/data$ps[data$pg == 2]
data$ipw[data$pg == 1] = 1/(1-data$ps[data$pg == 1])
```

The ATE estimate (and standard error) is
```{r, warning=FALSE, message=FALSE, cache=TRUE}
data_sex = data %>% filter(i_sex==1)
ATE1 = get_weighted_tau(data_sex$i_aqoc, data_sex$pg, 
                        data_sex$ipw, normalize = TRUE)
#SD1 = get_sd(data_sex$i_aqoc, data_sex$pg, 
#             data_sex$ipw, normalize = TRUE)
SD1 = get_sd(d=data, ps.model$formula, weight='ipw', 
             normalize=TRUE, subset=(data$i_sex==1))
cat(round(ATE1,4), paste0('(',round(SD1,4),')'),'\n')
```

### (b)
Estimate PS without `sex` as a variable in the logistic regression model, and then do IPW.
```{r}
ps.model = glm(pg ~ i_age + i_race + i_educ + 
                 i_insu + i_drug + i_seve + com_t + 
                 pcs_sd + mcs_sd, 
               data=data, family=binomial('logit'))
data$ps = ps.model$fitted.values %>% as.numeric()

data$ipw = 1
data$ipw[data$pg == 2] = 1/data$ps[data$pg == 2]
data$ipw[data$pg == 1] = 1/(1-data$ps[data$pg == 1])
```

The ATE estimate (and standard error) is
```{r, warning=FALSE, message=FALSE, cache=TRUE}
data_sex = data %>% filter(i_sex==1)
ATE2 = get_weighted_tau(data_sex$i_aqoc, data_sex$pg, 
                        data_sex$ipw, normalize = TRUE)
#SD2 = get_sd(data_sex$i_aqoc, data_sex$pg, 
#             data_sex$ipw, normalize = TRUE)
SD2 = get_sd(d=data, ps.model$formula, weight='ipw', 
             normalize=TRUE, subset=(data$i_sex==1))
cat(round(ATE2,4), paste0('(',round(SD2,4),')'),'\n')
```


### (c)
Within the sub-group of `sex=1`, estimate PS with all variables except `sex` and then do IPW.
```{r}
data_sex = data %>% filter(i_sex==1)
ps.model = glm(pg ~ i_age + i_race + i_educ + 
                 i_insu + i_drug + i_seve + com_t + 
                 pcs_sd + mcs_sd, 
               data=data_sex, 
               family=binomial('logit'))

data_sex$ps = ps.model$fitted.values %>% as.numeric()

data_sex$ipw = 1
data_sex$ipw[data_sex$pg == 2] = 1/data_sex$ps[data_sex$pg == 2]
data_sex$ipw[data_sex$pg == 1] = 1/(1-data_sex$ps[data_sex$pg == 1])
```

The ATE estimate (and standard error) is
```{r, warning=FALSE, message=FALSE, cache=TRUE}
ATE3 = get_weighted_tau(data_sex$i_aqoc, data_sex$pg, 
                        data_sex$ipw, normalize = TRUE)
#SD3 = get_sd(data_sex$i_aqoc, data_sex$pg, 
#             data_sex$ipw, normalize = TRUE)
SD3 = get_sd(d=data_sex, ps.model$formula, weight='ipw', 
             normalize=TRUE, subset=NULL)
cat(round(ATE3,4), paste0('(',round(SD3,4),')'),'\n')
```

**(a)** and **(b)** give us similar results, while **(c)** produces a slightly different one.

I think, among the 3 possible options, **(a)** and **(c)** are correct, while **(b)** isn't. 
<!-- Note that the estimand is $\mathbb{E}(Y(1)-Y(2) \mid V=1)$, which is the difference in satisfaction probability between the two groups **within the sex=1 sub-population**. Only **(c)** is dealing with the correct target population when estimating the propensity score and doing the weighting, while the other two options are not. -->

Note that to ensure the validity of a method, the propensity score $e(X)$ should make the covariates $X$ independent of treatment $Z$ conditional on $e(X)$ and $V=1$ (denoted by condition **W**). In **(a)**, the PS model is fitted with $V$ as a covariate, thus making $X$ **and** $V$ independent of the treatment given $e(X)$, which satisfies condition **W** ; in **(c)**, the PS model is fitted conditioned on $V=1$, which also satisfies condition **W**. 

## Problem 3

**(c)** and **(e)** are correct. 

**(a)** is wrong because we can't take $\delta$ as the estimate of the log odds ratio where there is an interaction term between covariates $X$ and treatment $Z$ (unless the covariates $X$ are centered). **(b)** is wrong because we can't just estimate $p_z$ within each group with treatment $z$, but should rather use the outcome model to "impute" the unobserved potential outcomes. 

**(d)** is incorrect because odds ratio is a nonlinear function of $p1$ and $p2$, so we can't estimate it within each block and simply average over the block estimates (the expectation of a nonlinear function is **not** the function of expectation). 

<!-- **(e)** is not correct because the estimand is the **odds ratio**, and generally speaking, when using propensity score stratification, we should obtain an estimate for the target estimand and then combine across strata, rather than combine to obtain estimates for "intermediate" quantities (the $p_z$'s) and then compute an estimate for the estimand from those quantities. -->

Now implement **(c)** and **(e)**. 

First, obtain estimate and standard error (via bootstrap) using **(c)**.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
get_est_c <- function(d){
  outcome.model = glm(i_aqoc ~ (i_age + i_sex + i_race + i_educ + 
                                  i_insu + i_drug + i_seve + com_t + 
                                  pcs_sd + mcs_sd) * pg, 
                      data=d, family=binomial('logit'))
  new = d
  new$pg[new$pg==2] = 1
  p1 = 1-mean(predict.glm(outcome.model, newdata = new, type = 'response'))
  new$pg[new$pg==1] = 2
  p2 = 1-mean(predict.glm(outcome.model, newdata = new, type = 'response'))
  
  #cat('p1=',p1,'p2=',p2,'\n')
  
  p1 * (1-p2) / (p2 * (1-p1))
}

get_sd_c <- function(d, B=2000){
  n = nrow(d)
  boot = sapply(1:B, function(i) {
    d.samp = d[sample(n,replace = TRUE),]
    
    # re-adjust the factors
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x)) factor(x) else x)
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x) & length(levels(x))<2) as.numeric(x) else x)
    
    get_est_c(d.samp)
  })
  
  sd(boot)
}
```

```{r, eval=FALSE, warning=FALSE, message=FALSE,cache=TRUE}
set.seed(79)
est.c = get_est_c(data)
sd.c = get_sd_c(data)

cat('Using method (c): \n Estimate =', est.c,
    'SD =', sd.c, '\n\n')
```

```{r, echo=FALSE}
load('c_estimates.RData')
cat('Using method (c): \n Estimate =', est.c,
    'SD =', sd.c, '\n\n')
```


Then use the method in **(e)**. (Still, obtain the estimate and then calculate standard errors via bootstrap.)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
get_est_e <- function(d, plot=FALSE){
  d[,c('i_age','com_t','pcs_sd','mcs_sd')] =
    scale(d[,c('i_age','com_t','pcs_sd','mcs_sd')],
          scale=FALSE)

  ps.model = glm(pg ~ i_age + i_sex + i_race + i_educ + 
                   i_insu + i_drug + i_seve + com_t + 
                   pcs_sd + mcs_sd, 
                 data=d, family=binomial('logit'))
  
  d$ps = ps.model$fitted.values
  qs = quantile(d$ps, seq(0.2,0.8,by=0.2))
  
  #cat('\nPS quantiles:',qs,'\n')
  
  d = d %>% 
    mutate(strata = case_when(
      ps < qs[1]  ~ 1,
      ps < qs[2]  ~ 2,
      ps < qs[3]  ~ 3,
      ps < qs[4]  ~ 4,
      ps >= qs[4] ~ 5
    ))
  
  if (plot){
    print(
      ggplot(d, aes(x=pg,fill=i_aqoc)) +
        geom_bar(position='fill') +
        facet_wrap(~strata)
      )
  }
  
  
  # ORs = numeric(5)
  # for (i in 1:5){
  #   d_i = d %>% filter(strata==i)
  #   satis1 = d_i %>% filter(pg==1) %>% select(i_aqoc) %>% pull()
  #   #cat('length of satis1:', length(satis1),'\n')
  #   if (length(satis1) > 0){
  #       p1 = mean(satis1 == 1, na.rm=TRUE)
  #     }else{
  #       p1 = 1
  #     }
  #   if(p1 > 0.99){ p1 = 0.99 }else if(p1 < 0.01){ p1 = 0.01}
  #   satis2 = d_i %>% filter(pg==2) %>% select(i_aqoc) %>% pull()
  #   #cat('length of satis2:', length(satis2),'\n')
  #   if (length(satis2) > 0){
  #       p2 = mean(satis2 == 1, na.rm=TRUE)
  #     }else{
  #       p2 = 0
  #     }
  #   p2 = mean(satis2 == 1, na.rm=TRUE)
  #   if(p2 > 0.99){ p2 = 0.99 }else if(p2 < 0.01){ p2 = 0.01}
  #   OR_i = p1 * (1-p2) / (p2 * (1-p1))
  #   
  #   #cat('p1=',p1,'p2=',p2,'\n')
  #   #cat('OR=', OR_i,'\n')
  #   
  #   ORs = c(ORs, OR_i)
  # }
  
  P1s = numeric(5); P2s = numeric(5)
  for (i in 1:5){
    d_i = d %>% filter(strata==i)
    satis1 = d_i %>% filter(pg==1) %>% select(i_aqoc) %>% pull()
    #cat('length of satis1:', length(satis1),'\n')
    # if (length(satis1) > 0){
    #     p1 = mean(satis1 == 1, na.rm=TRUE)
    #   }else{
    #     p1 = 1
    #   }
    # if(p1 > 0.99){ p1 = 0.99 }else if(p1 < 0.01){ p1 = 0.01}
    P1s = c(P1s, mean(satis1==1))
    
    satis2 = d_i %>% filter(pg==2) %>% select(i_aqoc) %>% pull()
    #cat('length of satis2:', length(satis2),'\n')
    # if (length(satis2) > 0){
    #     p2 = mean(satis2 == 1, na.rm=TRUE)
    #   }else{
    #     p2 = 0
    #   }
    # p2 = mean(satis2 == 1, na.rm=TRUE)
    # if(p2 > 0.99){ p2 = 0.99 }else if(p2 < 0.01){ p2 = 0.01}
    # OR_i = p1 * (1-p2) / (p2 * (1-p1))
    P2s = c(P2s, mean(satis2==1))

    #cat('p1=',mean(satis1==1),'p2=',mean(satis2==1),'\n')
    #cat('OR=', OR_i,'\n')

    #ORs = c(ORs, OR_i)
  }
  
  p1 = mean(P1s,na.rm=T); p2 = mean(P2s,na.rm=T)
  p1 * (1-p2) / (p2 * (1-p1))
}

get_sd_e <- function(d, B=1000){
  n = nrow(d)
  boot = sapply(1:B, function(i) {
    d.samp = d[sample(n,replace = TRUE),]
    
    # re-adjust the factors
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x)) factor(x) else x)
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x) & length(levels(x))<2) as.numeric(x) else x)
    
    get_est_e(d.samp)
  })
  
  sd(boot)
}
```

```{r, eval=FALSE, warning=FALSE, message=FALSE,cache=TRUE}
set.seed(83)

est.e = get_est_e(data)
sd.e = get_sd_e(data)

cat('Using method (e): \n Estimate =', est.e,
    'SD =', sd.e, '\n\n')
```

```{r, echo=FALSE}
load('e_estimates.RData')
cat('Using method (e): \n Estimate =', est.c,
    'SD =', sd.c, '\n\n')
```