---
title: "STA640 Homework 3"
author: "Fan Bu"
date: "10/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(foreign)
library(tidyverse)
library(PSweight)
library(tableone)
library(survey)
library(reshape2)
```


## Problem 1

Load the data first (I also checked that there is no missing values). 

```{r load data}
debit = read.dta("debitcard199598.dta")
```

Since we have `spending1995`, we can use it as the "lagged outcome" to test for unconfoundedness. Fit a linear model with all the main effects, the treatment variable (`debit_card1998`) and interactions between treatment and all other covariates:
$$
Y_{i, \text{lag}} \sim \beta X_{i,\text{no lag}} + \delta Z_i + \gamma X_{i,\text{no lag}} *Z_i,
$$
where $X_{i,\text{no lag}}$ consists of all the covariates except `spending1995`. We want to test if $\delta=0$, which implies 
$$
\mathbb{E} (Y_{i, \text{lag}}(Z_i=1) - Y_{i, \text{lag}}(Z_i=0) \mid X_{i,\text{no lag}} = x) = 0
$$
for all configurations $x$ of covariates. 

```{r check unconfound}
debit_lag = debit %>% select(-spending1998)
lag.ml = lm(spending1995 ~ . + debit_card1998 * (.), data=debit_lag)
summary(lag.ml)$coefficients['debit_card19981 (Yes)',]
```

Given the summary output, we shouldn't reject the null that $\delta=0$, which means we can accept the unconfoundedness assumption.

## Problem 2

Now check for overlap and balance. 

First check for balance by calculating the standard mean difference (i.e., the 2nd version of ASD with heurisic threshold at $0.1$) for each covariate, which yields the Love plot below.

We can see that there is severe imbalance on `average_age`, `householder_age` and `family_size`, among quite a few unbalanced covariates overall.
```{r directly check balance}
data_lag <- svydesign(ids = ~ 1, data = debit_lag, weights = ~ 1)

vars = names(debit_lag)[2:14]

## Construct a table that checks balance
tabOrig <- svyCreateTableOne(vars = vars, strata = 'debit_card1998',
                                 data = data_lag, test = FALSE)
## Show table with SMD
## (no output due to space limit)
#print(tabOrig, smd = TRUE)
```

```{r make Forrest plot of SMD, warning=FALSE, message=FALSE}
SMD = data.frame(SMD = as.numeric(ExtractSmd(tabOrig)),
                 Variable = vars)
SMD = SMD %>% arrange(desc(SMD))
## Order variable names by magnitude of SMD
varNames <- as.character(SMD$Variable)[order(SMD$SMD)]
## Order factor levels in the same order
SMD$Variable <- factor(SMD$Variable, levels = varNames)

ggplot(data = SMD,
       mapping = aes(x = Variable, y = SMD)) +
    #geom_line() +
    geom_hline(yintercept = 0.1, color = "black", size = 0.3) +
    geom_point(color='blue') +
    coord_flip() +
    theme_bw() + theme(legend.key = element_blank())
```

And then estimate the propensity score and plot its histograms for two groups.

From the histogram below, we can see that **there is some overlap between treated and control**, but there are some units with very low propensity scores (indicating not-great balance, which consolidates what we've found through SMD calulation).

```{r PS}
# use the data that doesn't contain outcome
# this PS model is chosen by trying out main effects and interactions
# until reasonably good balance is achieved
ps.model = glm(debit_card1998 ~ cash_inventory + interest_rate +
                 wealth + income + spending1995 + num_of_banks +
                 num_of_inhabitants + family_size + num_of_earners +
                 average_age + geograph_area + householder_age + 
                 householder_education + 
                 householder_age * householder_education, 
               data=debit_lag, family=binomial('logit'))
PS = ps.model$fitted.values
```

```{r check PS model, eval=FALSE, echo=FALSE}
debit_lag$ps = PS

debit_lag = debit_lag %>% 
  mutate(ipw = case_when(debit_card1998 == "1 (Yes)" ~ 1/ps,
                         debit_card1998 == "0 (No)" ~ 1/(1-ps)))

data_lagIPW <- svydesign(ids = ~ 1, data = debit_lag, weights = ~ ipw)
vars = names(debit_lag)[2:14]

## Construct a table that checks balance
tabIPW <- svyCreateTableOne(vars = vars, strata = 'debit_card1998',
                                 data = data_lagIPW, test = FALSE)
## Show table with SMD
print(tabIPW, smd = TRUE)
```

```{r plot PS scores}
debit_lag$ps = PS
ggplot(data=debit_lag, aes(x=ps,fill=debit_card1998)) +
  geom_histogram(bins = 20, position='dodge') + 
  theme_bw()
```


## Problem 3
Estimate the ATT.

### (a) Outcome regression
Fit a linear model with main effects of all the covariates, the treatment, and all possible interaction terms between treatment and covariates. Then the estimator for ATT would be
$$
\hat\tau = \frac{1}{N_1} \sum_{i=1}^N \mathbf{1}(Z_i=1) (Y_i - \hat{m}_0(X_i)).
$$
I'll use bootstrap to obtain the standard error of this estimate.

```{r outcome reg}
get_or_estimate <- function(dat){
  or = lm(spending1998 ~ . + debit_card1998 * (.), data=dat)
   
  dat_trt = dat %>% filter(debit_card1998 == "1 (Yes)")
  spending_trt = dat_trt$spending1998
  
  #dat_trt = dat_trt %>% dplyr::select(-spending1998)
  dat_trt = dat_trt[,-1]
  dat_trt$debit_card1998 = "0 (No)"
  
  spending_trt0 = predict(or, dat_trt)
  
  mean(spending_trt - spending_trt0)
}

get_or_sd <- function(dat, B=2000, seed=42){
  set.seed(seed)
  N = nrow(dat)
  estimates = sapply(1:B, function(i) {
    # bootstrap
    d.samp = dat[sample(1:N, replace = T),]
    
    # re-adjust the factors
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x)) factor(x) else x)
    d.samp[] <- lapply(d.samp, function(x) if(is.factor(x) & length(levels(x))<2) as.numeric(x) else x)
    
    get_or_estimate(d.samp)
  })
  
  sd(estimates)
}
```

```{r OR est and sd, cache=TRUE, message=FALSE, warning=FALSE}
Est = get_or_estimate(debit)
SD = get_or_sd(debit)
cat('Estimate:', Est,'\nStandard error:', SD,'\n')
```

### (b) Weighting estimator
Here I'm going to be lazy and use the `PSweight` package. Since the estimand is ATT, the weights on treated units and control units should be
$$
\begin{align}
w_1(x_i) &\propto 1, \\
w_0(x_i) &\propto \frac{\hat{e}(x_i)}{1-\hat{e}(x_i)}.
\end{align}
$$

Here $\hat{e}(x_i)$ will be estimated using the PS model that I have chosen earlier on. The standard error is estimated via bootstrap (with 1000 replicates).

```{r weighting, warning=FALSE, message=FALSE, cache=TRUE}
ps.formula = debit_card1998 ~ cash_inventory + interest_rate +
                 wealth + income + spending1995 + num_of_banks +
                 num_of_inhabitants + family_size + num_of_earners +
                 average_age + geograph_area + householder_age + 
                 householder_education + 
                 householder_age * householder_education
PS.fit = PSweight(ps.formula = ps.formula,
                  zname = "debit_card1998",
                  yname = "spending1998",
                  weight = "treated",
                  data = debit,
                  bootstrap = TRUE,
                  R = 1000)
```

Then check out the estimate, standard error, as well as 95% confidence interval. 
```{r weighting summary}
summary(PS.fit)
```


### (c) Double robust estimator

### (d) Bayesian outcome regression

Fit a Bayesian linear model to regress outcomes on covariates in the treated group and control group respectively. Here I'll use the `rstanarm` package to do Bayesian linear regression; as the default setting of this package, a flat prior ($Unif(0,1)$) on $R^2$ is adopted for the linear model. 

```{r, message=FALSE, warning=FALSE}
library(rstan)
library(rstanarm)
```

Split the dataset into the treated arm and control arm, and then fit a Bayesian LM model on each subset. The fitted models can then be used to impute $Y_i(0)$ as well as sample $Y_i(1)$ for each treated unit; the posterior parameter draws can also be directly used to compute estimates for the ATT estimand $\tau$ (more details are stated later on).

```{r}
Yes = debit %>% filter(debit_card1998 == "1 (Yes)") %>%
  select(-debit_card1998)

No = debit %>% filter(debit_card1998 == "0 (No)") %>%
  select(-debit_card1998)
```

```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
yes.fit = stan_lm(spending1998 ~ ., data=Yes, prior = R2(0.6), seed=42)
Y1 = Yes$spending1998
Y1_fit = posterior_predict(yes.fit)
sigma1 = as.matrix(yes.fit)[,'sigma']
```

```{r, include=FALSE}
#prior_summary(yes.fit)
```

```{r, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
no.fit = stan_lm(spending1998 ~ ., data=No, prior = R2(0.6), seed=42)
Y0 = No$spending1998
#Y0_fit = posterior_predict(no.fit)
Y0_predict = posterior_predict(no.fit, newdata = Yes)
sigma0 = as.matrix(no.fit)[,'sigma']
```

#### (i) Only draw $Y_i(0)$ from posterior for each treated unit $i$

At the same time, also vary $\rho$ from 0 to 1 (by an interval of 0.1). It seems that the result isn't very sensitive to the choice of $\rho$.

```{r}
# for treated units, draw Y(0)'s and get the mean
get_Y0 <- function(rho=0.5){
  nsamp = length(sigma0)
  N1 = nrow(Yes)
  #samps = numeric(n)
  mu = Y0_predict + rho * sigma0/sigma1 * (Y1 - Y1_fit)
  sigma = sigma0 * sqrt(1-rho^2)
  Y0_means = sapply(1:nsamp, function(i){
    mean(rnorm(N1, mu[i,], sigma[i]))
  })
  Y0_means
}

# method (i)
for(rho in seq(0.0,1.0,by=0.1)){
  Y0_means = get_Y0(rho)
ATT1 = mean(Y1) - Y0_means
cat('For rho =', round(rho,1), 'Estimate:', mean(ATT1), 
    '\n\t95% CI: [', paste0(round(quantile(ATT1, c(.025,.975)),4), 
                            collapse = ", "), ']\n')
}
```

#### (ii) Draw both $Y_i(0)$ and $Y_i(1)$ from posterior for each treated unit $i$

Same as before, vary the value of $\rho$ from 0 to 1. Still, the result isn't much affected by the choice of $\rho$. 

```{r}
# for treated units, draw Y(1) as well
Y1_means = apply(Y1_fit, 1, mean)

# method (ii)
for(rho in seq(0.0,1.0,by=0.1)){
  Y0_means = get_Y0(rho)
  ATT2 = Y1_means - Y0_means
  cat('For rho =', round(rho,1), 'Estimate:', mean(ATT2), 
      '\n\t95% CI: [', paste0(round(quantile(ATT2, c(.025,.975)),4), collapse = ", "), ']\n')
}
```

#### (iii) Express $\tau$ in terms of model parameters $\theta$ directly

Given that
$$
\begin{pmatrix}
Y_i(1)\\
Y_i(0)
\end{pmatrix}
\sim \mathcal{N} \left(
\begin{pmatrix}
\beta_1^T X_i\\
\beta_0^T X_i
\end{pmatrix},
\begin{pmatrix}
\sigma^2_1 & \rho \sigma_1\sigma_0\\
\rho \sigma_1\sigma_0 & \sigma^2_0
\end{pmatrix}
\right),
$$

we have $\delta_i = Y_i(1) - Y_0(1)$ follows
$$
\delta_i \sim \mathcal{N}(\beta_1^T X_i - \beta_0^T X_i, \sigma^2_1+\sigma^2_0-2\rho \sigma_0 \sigma_1).
$$

Then our estimand is
$$
\tau = \frac{1}{N} \sum_{i: Z_i=1} \delta_i.
$$

Now draw posterior predictive samples of $\tau$ by drawing samples of the parameters from their posterior. Again, vary $\rho$ from 0 to 1; the point estimate isn't affected by $\rho$ but the standard error is (not considerably though).
```{r}
get_delta_mean <- function(rho=0.5){
  nsamp = length(sigma0)
  N1 = nrow(Yes)
  Diffs = Y1_fit - Y0_predict
  diff_sigma = sqrt(sigma0^2 + sigma1^2 - 2*rho*sigma1*sigma0)
  delta_means = sapply(1:nsamp, function(i){
    mean(rnorm(N1, Diffs[i,], diff_sigma[i]))
  })
  delta_means
}

# method (iii)
for(rho in seq(0.0,1.0,by=0.1)){
  delta_means = get_delta_mean(rho)
  cat('For rho =', round(rho,1), 'Estimate:', mean(delta_means), 
      '\n\t95% CI: [', paste0(round(quantile(delta_means, 
                                             c(.025,.975)),4), 
                              collapse = ", "), ']\n')
}
```

#### Comparing the 3 methods
**(i)** is estimating the **sample** ATT, while **(ii)** and **(iii)** are closer to estimating the **population** ATT. However here we are doing these through a hybrid approach by conditioning on the sample distribution of covariates $X$ instead of modeling the distribution of $X$ within the treated group.

Also, it is notable that the latter 2 methods yield larger variances (wider credible intervals), since in **(i)** uncertainty only comes from imputing $Y_i(0)$'s, but in **(ii)** and **(iii)** uncertainty comes from "imputing" both potential outcomes.
