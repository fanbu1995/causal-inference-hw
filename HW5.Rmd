---
title: "STA640 Homework 5"
author: "Fan Bu"
date: "11/3/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

## PART 1

Load the data and required packages first

```{r data}
library(tidyverse)
library(ggplot2)
set.seed(42)

dat = read.delim('data-ps5.txt', sep = ' ')
dat = dat %>% drop_na() # turns out there's no missing data...
#glimpse(dat)
```

Note that since every variable is binary, our estimand is
$$
\tau = \mathbb{E}(Y(1,1) - Y(0,0)) = Pr(Y(1,1)=1) - Pr(Y(0,0)=1).
$$

### (a)
Using the formula shown in the lecture slides (here we also need to adjust for the baseline covariate $X_1$), we have
$$
\hat{Pr}(Y(1,1)=1) = \sum_{X_2^{obs}=0,1} \hat{Pr}(Y^{obs} \mid W_1 = 1, W_2 = 1, X_2^{obs}, X_1) \hat{Pr}(L^{obs} \mid W_1 = 1, X_1),
$$
and 
$$
\hat{Pr}(Y(0,0)=1) = \sum_{X_2^{obs}=0,1} \hat{Pr}(Y^{obs} \mid W_1 = 0, W_2 = 0, X_2^{obs}, X_1) \hat{Pr}(X_2^{obs} \mid W_1 = 0, X_1).
$$

<!-- We use R to calculate these two values, which are -->
<!-- ```{r} -->
<!-- # code is kinda convoluted, but got OCD with tidyverse... -->
<!-- p1 = dat %>% filter(w1==1, w2==1) %>% -->
<!--   group_by(x2) %>% -->
<!--   summarize(prob_y1 = mean(y), count_x2 = n()) %>% -->
<!--   mutate(prob_x2 = count_x2/sum(count_x2)) %>% -->
<!--   summarize(p1 = sum(prob_y1 * prob_x2)) %>% -->
<!--   pull() -->

<!-- p2 = dat %>% filter(w1==0, w2==0) %>% -->
<!--   group_by(x2) %>% -->
<!--   summarize(prob_y1 = mean(y), count_x2 = n()) %>% -->
<!--   mutate(prob_x2 = count_x2/sum(count_x2)) %>% -->
<!--   summarize(p1 = sum(prob_y1 * prob_x2)) %>% -->
<!--   pull() -->

<!-- cat('Pr(Y(1,1)=1) =',p1, ' Pr(Y(0,0)=1) =', p2, '\n') -->
<!-- ``` -->


<!-- And then our estimate is -->
<!-- $$ -->
<!-- \hat\tau = \hat{Pr}(Y(1,1)=1) - \hat{Pr}(Y(0,0)=1) -->
<!-- $$ -->
<!-- which turns out to be -->
<!-- ```{r} -->
<!-- p1 - p2 -->
<!-- ``` -->


We do the following things:

1. Fit a logistic regression model for $Y$ on the treatment sequence, the intermidiate outcome and baseline covariate: 
(Note: I tried to add in some interaction terms but they don't seem to lead to a better fit, so I'll just keep it simple.)
```{r}
mod.Y = glm(y ~ . , data=dat, family = 'binomial')
```

2. Fit a logistic regression model for $X_2$ on the treatment at time $1$ and the baseline covariate:
```{r}
mod.x2 = glm(x2 ~ w1 + x1, data = dat, family = 'binomial')
```

3. Simulate $S = 1000$ times from the fitted model to get Monte Carlo estimates of $Pr(Y(1,1)=1)$ and $Pr(Y(0,0)=1)$:
```{r}
# expit function
expit <- function(x){ exp(x)/(1+exp(x)) }

# simulate bernoulli from log odds
sim_bern_logit <- function(x, N){ as.numeric(rbernoulli(N, expit(x))) }

# do it (resample from original data to get the baseline covars)
S = 1000
N = nrow(dat)
samp = dat %>% slice(sample(N, S, replace = TRUE))

## A = (1,1)
samp11 = samp %>% mutate(w1 = 1, w2 = 1)
samp11$x2 = sim_bern_logit(predict(mod.x2, newdata = samp11), S)
samp11$y = expit(predict(mod.Y, newdata = samp11))
p11 = mean(samp11$y)

## A = (0,0)
samp00 = samp %>% mutate(w1 = 0, w2 = 0)
samp00$x2 = sim_bern_logit(predict(mod.x2, newdata = samp00), S)
samp00$y = expit(predict(mod.Y, newdata = samp00))
p00 = mean(samp00$y)
```

4. Calculate $\hat{\tau}$, which is
```{r}
p11 - p00
```


### (b)
Do the following things:

1. Specify a model for outcome $Y$ under "randomization":
$$
\text{logit}(Pr(Y=1)) \sim W_1 + W_2 + W_1 \times W_2. 
$$

2. Build propensity score models for time $1$ and $2$

```{r}
ps1 = glm(w1 ~ x1, data=dat, family = 'binomial')
ps2 = glm(w2 ~ x1 + w1 + x2, data=dat, 
          family = 'binomial') # adding interactions doesn't seem helpful
```

and also the unconditional probabilities of treatment assignments at both time points:
```{r}
up1 = glm(w1 ~ 1, data=dat, family = 'binomial') # a constant prob here
up2 = glm(w2 ~ w1, data=dat, family = 'binomial')
```

3. Estimate the propensity score for all units at each time point and check for overlap. 
```{r PS for two time points}
library(broom)

# time point 1
e1 = ps1$fitted.values

## check Pr(W1=1) for X1=0 and 1 separately
augment(ps1, newdata = data.frame(x1=c(0,1))) %>%
  mutate(fitted_ps = expit(.fitted)) %>%
  select(x1, fitted_ps)

# time point 2
e2 = ps2$fitted.values

e2_dat = data.frame(PS = e2, W2 = dat$w2)
ggplot(data=e2_dat, aes(x=e2, fill=factor(W2))) +
  geom_histogram(position = 'dodge') +
  labs(x='propensity score at time 2', fill='W2') +
  theme_bw()
```

It seems that at time $1$, there is some imbalance, but at time $2$ the imbalance is more severe. That being said, all the fitted propensity scores are not very extreme (all within [0.05, 0.96]) so I will not truncate any. 

4. Calculate stabilized weights for all units.
```{r}
sw = data.frame(e1 = e1, e2 = e2, up1 = up1$fitted.values, 
                up2 = up2$fitted.values, 
                w1 = dat$w1, w2 = dat$w2) %>%
  mutate(ipw1_inv = ifelse(w1==1, e1, 1-e1), 
         ipw2_inv = ifelse(w2==1, e2, 1-e2)) %>%
  mutate(sw = (up1 * up2)/(ipw1_inv*ipw2_inv))
```

5. Fit the weighted outcome model (specified in 1).
```{r}
sms.mod = glm(y ~ w1 * w2, data=dat, weights = sw$sw, 
              family = 'binomial')
```

And finally, obtain an estimate for $\tau$ using the fitted model:
```{r}
# (0,0) and (1,1)
contra = data.frame(w1=c(0,1), w2=c(0,1))
# calculate the estimated Pr(Y(1,1)=1) - Pr(Y(1,1)=1)
cat('Estimate for tau:\n')
augment(sms.mod, newdata = contra) %>%
  mutate(prob = expit(.fitted)) %>%
  summarise(tau = diff(prob)) %>%
  select(tau) %>%
  pull()
```

### (c)
Do the following things (according to the steps in Section 3 of Keil et al., 2018).

1. Specify the joint model for $(x_t, w_t, y)$ ($t=1,2$) for the target population:

- a model for $X_2$ (intermediate outcome), determined by $p^{(x_2)}(x1,w1) = Pr(X_2 \mid X_1 = x_1, W_1 = w_1)$ (4 probabilities).

- a model for $Y$ (outcome), determined by $p^{(y)}(x1,w1,x2,w2) = Pr(Y \mid X_1 = x_1, W_1 = w_1, X_2 = x_2, W_2 = w_2)$ (16 probabilities).

2. Specify the priors: $Unif(0,1) = Beta(1,1)$ for each probability in the above.

3. Sample from the target population via $p(X_1)$; here we simply use the empirical estimate for $Pr(X_1=1)$ in our sample data, which is `r mean(dat$x1)`.
```{r}
p_x1 = mean(dat$x1)
```

4. Set the treatment sequences (that we care about); they are $(W_1, W_2) = (1,1)$ and $(W_1,W_2) = (0,0)$.

5. Draw from the posterior distribution of parameters; note here we can utilize the Beta-Binomial conjugacy, and so with independent $Unif(0,1)$ priors, the posterior distribution of the probability (of getting a $1$) in each cell is essentially $Beta(1+\text{num. of 1s}, 1+\text{num. of 0s})$.
```{r}
# number of samples 
S = 5000

# get X1's from p_x1
X1s = rbernoulli(S, p=p_x1) %>% as.numeric()

# the posteriors for X_2 probs
post_X2 = dat %>% count(x1,w1,x2) %>%
  mutate(post = n+1)

# posterior samples for X_2 probs (for w1=0 and w1=1)
get_probs_X2 <- function(x1_vec, w){
  res = numeric(length(x1_vec))
  n0 = sum(x1_vec==0)
  n1 = sum(x1_vec==1)
  
  a0 = post_X2 %>% filter(x1==0, w1==w, x2==1) %>%
    select(post) %>% pull()
  b0 = post_X2 %>% filter(x1==0, w1==w, x2==0) %>%
    select(post) %>% pull()
  #cat(a0, b0, '\n')
  res[x1_vec==0] = rbeta(n0, a0, b0)
  
  a1 = post_X2 %>% filter(x1==1, w1==w, x2==1) %>%
    select(post) %>% pull()
  b1 = post_X2 %>% filter(x1==1, w1==w, x2==0) %>%
    select(post) %>% pull()
  res[x1_vec==1] = rbeta(n1, a1, b1)
  #cat(a1, b1, '\n')
  res
}

probs_X2 = list('0' = get_probs_X2(X1s, 0), '1' = get_probs_X2(X1s, 1))


# the posteriors for Y probs
post_Y = dat %>% count(x1,w1,x2,w2,y) %>%
  mutate(post = n+1)
```

6. Draw the posterior predictive samples and get posterior samples for $\tau = Pr(Y(1,1)=1) - Pr(Y(0,0)=1)$. 
```{r}
# sample p00 and p11 for Y
# given the X1 and X2 samples drawn

# draw X2 samples first
# under W1 = 0 and W1 = 1
X2s = list('0' = rbernoulli(S, p=probs_X2$`0`),
           '1' = rbernoulli(S, p=probs_X2$`1`))

# then draw probs of Y=1 under W=(0,0) and W=(1,1), conditioned on X1 and X2
get_probs_Y <- function(X1_vec, X2_vec, w_1, w_2){
  res = numeric(S)
  
  for(x_1 in c(0,1)){
    for(x_2 in c(0,1)){
      n_this = sum(X1_vec == x_1 & X2_vec == x_2)
      if(n_this > 0){
        a_this = post_Y %>% 
        filter(x1==x_1, w1==w_1, x2==x_2, w2==w_2, y==1) %>%
        select(post) %>% pull()
        # if no observed data in cell, set it to prior
        if(length(a_this)==0){ a_this = 1}
        b_this = post_Y %>% 
          filter(x1==x_1, w1==w_1, x2==x_2, w2==w_2, y==0) %>%
          select(post) %>% pull()
        # again, if no observed data in cell, set it to prior
        if(length(b_this)==0){ b_this = 1}
      
        res[X1_vec == x_1 & X2_vec == x_2] = rbeta(n_this, a_this, b_this)
      }
    }
  }
  res
}

probY_00 = get_probs_Y(X1s, X2s$`0`, 0,0)
probY_11 = get_probs_Y(X1s, X2s$`1`, 1,1)
```

Here we report posterior mean as well as a 95\% credible interval for $\tau$:
```{r}
Bayes_tau = probY_11 - probY_00
cat('Posterior mean:', mean(Bayes_tau), '\n95% CI:', quantile(Bayes_tau,c(.025, .975)))
```

### (d)
In the context of this problem, the joint model of all variables can be factorized as 
$$
\begin{aligned}
&p(X_1, W_1, X_2, W_2, Y)\\
=& p(Y \mid X_1, W_1, X_2, W_2) p(W_2 \mid X_1, W_1, X_2) p(X_2 \mid X_1, W_1) p(W_1 \mid X_1) p(X_1).
\end{aligned}
$$

Then I need to specify 5 models in total: 

1. model for $Y$ (outcome) given $X_1, W_1, X_2, W_2$
2. model for $W_2$ (2nd treament assignment) given $X_1, W_1, X_2$
3. model for $X_2$ (intermediate outcome) given $X_1, X_1$
4. model for $W_1$ (1st treament assignment) given $X_1$
5. model for $X_1$ (the target population distribution)

Here all of them can be (saturated) binary outcome models, and again I can adopt independent $unif(0,1)$ priors for all the cell probabilities. Estimation for $\tau$ can be done by drawing from the posterior distributions of the parameters. 


## PART 2
For any treatment sequence $(a_1, a_2, a_3)$, we have
$$
\begin{aligned}
&Pr(Y(a_1,a_2,a_3)=1)\\
=& \sum_{(X_1^{obs},X_2^{obs})\in \mathcal{X}} Pr(Y^{obs}=1 \mid W_1 = a_1, X_1^{obs}, W_2 = a_2, X_2^{obs}, W_3 = a_3)\\
& \qquad \qquad \times Pr(X_1^{obs} \mid W_1 = a_1) \\
& \qquad \qquad \times Pr(X_2^{obs} \mid W_1 = a_1, X_1^{obs}, W_2 = a_2),
\end{aligned}
$$
where $\mathcal{X} = \{(0,0),(0,1), (1,0), (1,1)\}$ is the set of all possible combinations of $(X_1^{obs},X_2^{obs})$. 

Then to estimate $\tau = \mathbb{E} (Y(1,1,1) - Y(0,0,0))$ we will first estimate $Pr(Y(1,1,1) = 1)$ and $Pr(Y(0,0,0) = 1)$ and then take the difference.

Note that for treatments $(1,1,1)$, we only have $(X_1^{obs},X_2^{obs}) = (1,1)$, so
$$
Pr(Y(1,1,1) = 1) = 60\% \times 100\% \times 100\% = 60\%.
$$
Then for treatments $(0,0,0)$, we need to sum over all four combinations:
$$
\begin{aligned}
&Pr(Y(0,0,0) = 1)\\
=& 0 + 40\% \times 50\% \times 50\% + 40\% \times 50\% \times 50\% + 60\% \times 50\% \times 50\%\\  
=& 35\%. 
\end{aligned}
$$
Therefore our estimate for the causal effect is
$$
\hat{\tau} = 60\% -35\% = 25\%.
$$

